---
title: "ML_2_Summary"
author: "Study Crew"
date: "3/9/2021"
output:
    html_document:
    toc: true
    toc_float: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Machine Learning 2 Summary Document {.tabset}

This document is a comprehensive summary for Machine Learning 2. It is intended to help us study for the exams and for future reference in our professional lives (if any companies actually use R).

## Basic Statistics {.tabset}
Authors: Carrington Metts

### Bias/Variance Tradeoff
**Variance** is defined as the amount by which a prediction would vary if it were estimated with a different training set. A model with high variance is *overfit*- small 
changes in the training set may result in large changes to the model. 

**Bias** is defined as the amount of error that is due to reducing a complex real-world dataset to a simple model. A model with high bias is *underfit*- changes in the 
training set are unlikely to significantly change the model, but the model still does not make good predictions for the data. 

If your training error is low and test error is high, the model has *high variance*. If both the training and testing errors are high, the model has 
*high bias*. If both the training and testing errors are low, the model has *low variance and low bias*. 

## Forward and Backward Selection {.tabset}
Author: Matt Sadosuk

### Summary/description:

For forward stepwise selection: The model begins with no variables, then starts
adding the most significant variables until a pre-specified stopping rule is reached 

For backward stepwise selection: it begins with a model that contains all variables under
consideration, then starts removing the least significant variables until a 
pre-specified stopping rule is reached or until no vraible is left in the model


### When to employ this method/model:
Forward and backward selection will be used when youre trying to identify the amount
of features that will produce the best model.

This R script will cover how to use Forward and Backward selection to identify which model is the best.
As well it will feature a validation process to identify how close the results were before performing the test
on the data set.

Using the library mass, use the Boston dataset which is about the housing suburbs of the Boston area
```{r}
library(MASS)

data(Boston)

length(Boston)
```

The next step involves subsetting the data through the function regsubset from the library (leaps).
First we need to perform the test without forward and backward selection to see how the model looks.
Then to note for the code we will use medv (median value of owner-occupied homes in $1000s) as our Y-var,
nvmax =14 which is the number of columns, then use the methods "forward" and "backward"

```{r}
library(leaps)

regfit.full=regsubsets(medv~.,data=Boston ,nvmax=14)
reg.summary =summary(regfit.full)

regfit.fwd=regsubsets(medv~.,data=Boston , nvmax=14, method ="forward")
summary(regfit.fwd)

regfit.bwd=regsubsets(medv~.,data=Boston , nvmax=14, method ="backward")
summary(regfit.bwd)

```

Following the subsetting the next step is to do coefficient test on the best 7 x variables. 


```{r}
reg.summary$rsq

coef(regfit.full ,7)
coef(regfit.fwd ,7)
coef(regfit.bwd ,7)
```

In order for these approaches to yield accurate estimates of the test
error, we must use only the training observations to perform all aspects of
model-fitting—including variable selection. Therefore, the determination of
which model of a given size is best must be made using only the training
observations. 

Now we split the training and test data using a sample vector of true and false items. 
If true item is in the training set otherwise it will be False. The test
is TRUE if the observation point is in the data set otherwise it will return False. 
In the test line we use the !train to switch the true and false values for the 
test dataset

```{r}
set.seed(1)
train=sample(c(TRUE ,FALSE), nrow(Boston),rep=TRUE)
test=(!train)
```

Following the splitting of data into the test and training set we now want to 
perform the best subset selection. We do this by now using Boston[train,] which
is new sample vector of true and false values. We are still using a nvmax of 14. 

```{r}
regfit.best=regsubsets(medv~.,data=Boston[train ,],nvmax=14)
```

Next we will get the validation error for the best model size by creating a model
matrix for the test data. The model.matrix() function builds an X matrix from the data

```{r}
test.mat=model.matrix(medv~.,data=Boston[test ,])
```

Next we create a for loop that takes each size i and will be extract the coefficients
from the variable regfit.best, then multiply the coef into the appropriate columns
for the test model matrix to make predictions and the compute the test MSE

First we create a empty vector from 1 to 13, the create the for loop, inside the 
brackets calculate coefficient of each i, then make predictions from those coefficients
and finally using the empty vector of errors calculate the mse and fill the empty vector
with the new test MSE
```{r}
val.errors = rep(1,13)
for(i in 1:13){
  coefi=coef(regfit.best ,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((Boston$medv[test]-pred)^2)
}
val.errors
```

We now use the which min to see what is the best amount of variables to use
```{r}
which.min(val.errors)
```

```{r}
predict.regsubsets=function (object , newdata ,id ,...){
form=as.formula (object$call [[2]])
mat=model.matrix(form ,newdata )
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi
}
```
With the new variables test the new coefficients to see the new variables interaction
with the model

```{r}
regfit.best=regsubsets(medv~.,data=Boston ,nvmax=14)
coef(regfit.best,11)
```

Now we want to choose among models with different sizes using cross validation method.
We want to perform the best subset selection within each of the k training sets.
Since our best fit is 11 variables we will set k =11, next we will use the k fold 
method. This involves taking a sample of each of the k values for all the rows
in the boston dataset. Finally we will create a matrix to store the store the results


```{r}
k=11
set.seed(1)
folds=sample(1:k,nrow(Boston),replace=TRUE)
cv.errors= matrix(NA,k,14, dimnames = list(NULL, paste(1:14)))
```

Now we can perform the cross validation through a for loop that looks for elements 
of folds that equal j are in the test and the rest in the training. Which the 
folds! will accomplish that goal. After this we use another loop to make model
predictions and then calculate the test error rates for the subsetted data. 
Using the predict function we will be predicting using the best.fit model and 
using all be all of the folds that equal the jth element. Finally the cv errors
will index of each column and row values and fill the error vector with the 
new calculated mse values

```{r}
for(j in 1:k){
 best.fit=regsubsets(medv~.,data=Boston[folds!=j,],nvmax=14)
 for(i in 1:13){
  pred=predict(best.fit ,Boston[folds ==j,],id=i)
  cv.errors[j,i]= mean((Boston$medv[ folds==j]-pred)^2)
  }
}
```

Now we apply the cv.error mse values and and find the mean of each row and take the
square root of the values
```{r}
mean.cv.errors=sqrt(apply(cv.errors ,2, mean))
mean.cv.errors

par(mfrow=c(1,1))
plot(mean.cv.errors ,type="b")
```

After doing the CV on the Boston data set we can conclude that the 11 variable model
is the best fit
```{r}
reg.best=regsubsets(medv~.,data=Boston , nvmax=14)
coef(reg.best,11)
```

### Model Code and Inputs:

```{r}

```

### Model Tests:

```{r}

```

### Model Improvements:

```{r}

```

### Comprehensive Example:

```{r}

```
--------------------------------------------------------------------------------


## Ridge Regression {.tabset}
Author: Sarah Brown

### Summary/description:
With this method, you shrink the B coefficients of your features towards 0. It reduces variance but increases bias in the training data. It doesn't let you make a coefficient 0 (it shrinks asymptotically) and so no features are eliminated. So, we use this when most features are useful and none seem irrelevant (if they did, you'd look at using lasso perhaps). As λ goes up, beta slope goes down and the line gets flatter which means the model is less sensative to the features. This means that the weight decay in gradient descent is larger.

### When to employ this method/model:
When the the training model is very accurate, but the test model is not, signaling that there is a large least squares error (high variance) in the test model. This means that the line is overfit to the training data. You want to minimize the the cost function which is: RSS + λ * (slope)^2 [penalty] where λ is the severity of the penalty, and is a tuning parameter. You can use ridge regression with:

Multiple regression
    λ(slope12 + slope22 + ….) (except for intercept)

Categorical X (binary)
    Slope = difference between the two categories

Logistic Regression
    Categorical Y
    Optimizes sum of likelihoods (rather than RSS)
*Multiple Regression thru Logistic Regression above, is directly out of Professor Li's Ppt

### Model Code and Inputs:

```{r,eval = F}
#First, divide the data into training and testing (in this case, its 75% training)
# (6) Create the train (75%) and test (25%) data sets 
train = sample(1:nrow(x), nrow(x)*.75)
test=(-train)
y.test=y[test]

#Create a grid of lambda valaues. Then use the glmnet() function to create a model to predict the #training y's using the training x's. Use alpha = 0 for ridge regression.
grid=10^seq(10,-3, length =120)
mod.ridge <- glmnet(x[train,], y[train], alpha=0, lambda =grid)
```

### Model Tests:

``` {r,eval = F}
#Evaluate training model performance (how well it predicted the y-values) by using cross-validation, which is the cv.glmnet() function. (In this example code, we are creating a 12-fold cross-validation model, but by default it is a 10-fold cv).
cv.out.ridge <- cv.glmnet(x[train,], y[train], alpha=0, lambda = grid, nfolds = 12)
plot(cv.out.ridge)
```

### Model Improvements:

``` {r,eval = F}
#Make predictions using the best model by using the best lambda. Create a vector of test set #predictions.
bestlam_r <- cv.out.ridge$lambda.min
ridge.pred <- predict(mod.ridge, s=bestlam_r, newx=x[test,])

#Compute and display the test error rate.
MSE_best_ridge<-mean((ridge.pred - y.test)^2) 
```

### Comprehensive Example:

```{r,eval = F}
#Part 1: Exploring ridge regression without using a test and train set
#Using the hitters data set, we set up the "x" and "y" matrix values so that we can use the glmnet function. 
x=model.matrix(Salary~.,HitData)[,-1]
y=HitData$Salary

#Make a grid of lambda values, and see how many coefficients you have and how many potential lambda #values there are.
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha = 0,lambda = grid)
COEF<-coef(ridge.mod)
dim(COEF)
[1]  20 100 # 20 coefficients and 100 possible lambda values

#You can look at various lambda values and how they preform. Here is an example of looking at the #50th lambda value from the grid. 
ridge.mod$lambda[50]
[1] 11497.57
coef(ridge.mod)[,50]
  (Intercept)         AtBat          Hits         HmRun          Runs 
407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 
          RBI         Walks         Years        CAtBat         CHits 
  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 
       CHmRun         CRuns          CRBI        CWalks       LeagueN 
  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 
    DivisionW       PutOuts       Assists        Errors    NewLeagueN 
 -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 
sqrt(sum(coef(ridge.mod)[-1,50]^2))# Find the L2 value
[1] 6.360612

#Part 2: Predicting with a test and train set also using the hitters data
#Create test and train 
set.seed(1)
train<-sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]

#To choose the best lambda value, we use cv.glmnet. We can find the best lambda and associated MSE.
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam<-cv.out$lambda.min
bestlam #326.0828
[1] 326.0828
ridge.pred<-predict(ridge.mod,s=bestlam,newx = x[test,])
mean((ridge.pred-y.test)^2) # 139856.6
[1] 139856.6

#We can also explore what the coefficients look like using the whole data set and our best lamda #value
out<-glmnet(x,y,alpha = 0)
predict(out,type = "coefficients",s=bestlam)[1:20,]
 (Intercept)        AtBat         Hits        HmRun         Runs          RBI 
 15.44383135   0.07715547   0.85911581   0.60103107   1.06369007   0.87936105 
       Walks        Years       CAtBat        CHits       CHmRun        CRuns 
  1.62444616   1.35254780   0.01134999   0.05746654   0.40680157   0.11456224 
        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists 
  0.12116504   0.05299202  22.09143189 -79.04032637   0.16619903   0.02941950 
      Errors   NewLeagueN 
 -1.36092945   9.12487767 
# no coefficients are zero because in ridge regression they aren't allowed to be
```
--------------------------------------------------------------------------------


## Lasso Regression {.tabset}
Author: Jill Van den Dungen 

### Summary/description:
Lasso is an alternative to the ridge regression. In lasso the l1 penalty has the effect of forcing some of the coefficients estimates to be exactly equal to zero when the tuning parameter lambda is sufficiently large. Resulting in the elimination of certain variables. Lasso performs variable selection. Models generated from lasso are sparse = they are easier to interpret than the models by ridge regression. 

### When to employ this method/model:
Use lasso when we suspect that the model includes a lot of useless features. 


### Model Code and Inputs:

```{r, eval =F}
#Create the train (75%) and test (25%) data sets 
train<-sample(1:nrow(x), nrow(x)*0.75)
test<- (-train)
y_test=y[test]

#Create a lamba grid vector of 120 elements ranging from 10^10 to 10^-3 
grid=10^seq(10,-3,length=120)

#Using the glmnet() function, create a lasso model named mod.lasso that predicts the training y's using the training x's and the grid of lambda values created above.

mod.lasso<-glmnet(x[train,],y[train],alpha = 1,lambda = grid)

```

### Model Tests:

```{r,eval =F}
#Evaluate training model performance using cross-validation
#Using the cv.glmnet() function and the same parameters used above in the creation of mod.lasso (i.e. including the lambda grid vector),
#     create a 12-fold cross-validation model named cv.out.lasso

cv.out.lasso<-cv.glmnet(x[train,],y[train],alpha = 1,lambda = grid, nfolds=12)
plot(mod.lasso)
```

### Model Improvements:

```{r,eval =F}
#Display the best cross-validated lambda value (the one that produces the lowest deviance - do not use the 1-standard error rule here).
bestlam<-cv.out.lasso$lambda.min

#Using the best lambda and the model named mod.lasso, create a vector of test set predictions.
lasso.pred<- predict(mod.lasso, s=bestlam, newx= x[test,])

#Compute and display the test error rate.
mean((lasso.pred-y_test)^2)
```

### Comprehensive Example:

```{r,eval =F}
#Using the hitters data set, we set up the "x" and "y" matrix values so that we can use the glmnet function. 
x=model.matrix(Salary~.,HitData)[,-1]
y=HitData$Salary

#Make a grid of lambda values
grid=10^seq(10,-2,length=100)

#Create test and train 
set.seed(1)
train<-sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]

#Create lasso model using train set
lasso.mod<-glmnet(x[train,],y[train], alpha=1, lambda= grid)
plot(lasso.mod)

#Create cross validation model
set.seed(1)
cv.out<-cv.glmnet(x[train], y[train], alpha=1)
plot(cv.out)

#Create best lambda and make predictions
bestlam<- cv.out$lambda.train
bestlam

lasso.pred<-predict(lasso.mod, s=bestlam, newx= x[test,])
mean((lasso.pred-y.test)^2)

#explore whole data set to see what best lambda is
out=glmnet(x,y,alpha= 1, lambda= grid)
lasso.coef<- predict(out, type= 'coefficients', s=bestlam)[1:20,]
lasso.coef
 (Intercept)         AtBat          Hits         HmRun          Runs 
   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 
          RBI         Walks         Years        CAtBat         CHits 
   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 
       CHmRun         CRuns          CRBI        CWalks       LeagueN 
   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 
    DivisionW       PutOuts       Assists        Errors    NewLeagueN 
-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 

```
--------------------------------------------------------------------------------


## Principal Component Regression {.tabset}
Author: Alex King

### Summary/description:

Principal Components Analysis (PCA): Popular Algorithm for deriving a low-dimensional set of features from a large set of features from a large set of variables. This is done by transforming X features through linear combinations
	
There are two methods:

Principal Components Regression/Analysis
Partial Least Squares

Both involve supervised regression of Y on components. However, PCR is the unsupervised identification of components in the X feature space, while PLS is supervised.

PLS and PCR have very similar performances.



### When to employ this method/model:

Uses: 
1) When there are more variables than observation (wide data)
Shopping Behavior (Y) and Searh Keywords
2) When the explanatory variables are highly correlated
3) Use PLS when you want to reduce bias

Potential Drawbacks: There is no guarantee that the selected principal components are associated with the outcome. If there is no correlation with the variables, you will run into trouble. PLS can also increase variance


### Model Code and Inputs:
Library: PLS

PCR: pcr(Y~X, data=DF,scale=TRUE,validation= “cv”)
PLS: plsr(Y~X,data=df,scale=TRUE,validation= “cv”)


### Model Tests:

 For the model tests, we will use the Validation Plot Function. This function allows you to graph show the Mean Square Error of Prediction versus the # of Components. You will choose the number of components that has the lowest MSEP.

validationplot(pcr.fit, val.type="MSEP")


### Model Improvements:
Adjust nComp which is the number of principal components to build the models for. Adding Scale = True

### Comprehensive Example:

```{r}
#### PCR

### Step 1: Load Require Packages
library(tidyverse)
library(caret)
library(pls)

### Step 2: Load Data Set the Data
library(MASS)
data("Boston")
str(Boston)

### Step 3: Split Dataset
set.seed(123)
training.indices <- Boston$medv %>% createDataPartition(p=0.8,list=FALSE)
train <- Boston[training.indices,]
test <- Boston[-training.indices,]
y.test <- test$medv

### Step 4: Compute PCR

# Build Model
model <- pcr(medv~.,data=Boston,subset=training.indices,scale=TRUE,validation="CV")
summary(model)

validationplot(model,val.type = "MSEP")


### Calculate Lowest MSE
pcr.pred = predict(model,test,ncomp=13)
mean((pcr.pred-y.test)^2)
# MSE = 21.05844

### Test on whole Dataset
model.whole <- pcr(medv~.,data=Boston,scale=TRUE,validation="CV")
pcr.pred = predict(model.whole,test,ncomp=13)
mean((pcr.pred-y.test)^2)
# MSE = 19.934

##### PLS
### Steps 1-3 Same as PCR
pls.fit <- plsr(medv~.,data=Boston,subset=training.indices, scale=TRUE,validation="CV")
summary(pls.fit)

validationplot(pls.fit,val.type="MSEP")

pls.pred <- predict(pls.fit,test,ncomp=8)
mean((pls.pred-y.test)^2)
# MSE = 20.95983

### Whole Dataset
pls.whole <- plsr(medv~.,data=Boston, scale=TRUE,validation="CV")
pls.pred <- predict(pls.whole,test,ncomp=8)
mean((pls.pred-y.test)^2)
# MSE = 19.90098
```
--------------------------------------------------------------------------------


## Polynomial Regression {.tabset}
Author: Carrington Metts

### Summary/description:
Polynomial regression fits a polynomial curve to a set of data. It is analagous to linear regression, but allows greater flexibility 
to fit to data that does not have a linear relationship. 

### When to employ this method/model:
Polynomial regression should be used when there is a continuous response variable and one or more continuous features, 
and there is a nonlinear relationship between the X and Y variables. 

Polynomial regression may also be used to estimate a binary response variable, given a set of continuous features. 

### Model Code and Inputs:
A polynomial fit can be produced with either orthogonal or non-orthogonal terms. Having orthogonal polynomials means each 
term in the fit is a linear combination of each polynomial term (age^2, age^3, age^4, etc.) Non-orthogonal polynomials 
means the coefficients are reported directly. Both versions of orthogonality will result in the same fitted values. 
When creating models, **it is best to always use orthogonal terms**, since non-orthogonal terms are correlated with 
each other and therefore result in a multicollinearity problem. 

As with linear regression, we use lm() or glm() to create the fit. Within lm() or glm(), the poly() function automatically creates orthogonal
terms. (To create non-orthogonal terms, you can use the cbind() function, or set raw=True inside poly()). To create a model, we will use the Wage data (from ISLR) to predict
wage as a function of age. We can then use coef() to get the coefficients of the model. 

```{r poly_model, warning=FALSE}
library(ISLR)
#fit a degree-4 polynomial in 2 ways
orthogonal_fit <- lm(wage~poly(age,4), data=Wage)
summary(orthogonal_fit)

```

We can also create a polynomial logistic regression model to predict whether a person will earn more than $250,000. To do so,
we use I() to coerce the continuous wage variable to binary categorical. We then run a glm fit on the model. 

```{r poly_binary_model}
logistic_fit <- glm(I(wage>250)~poly(age, 4), data=Wage, family=binomial)
```



### Model Tests:
It is important to choose the lowest-degree model that adequately models the data. To do so, we use the ANOVA (analysis of variance)
test. ANOVA computes an F-test against two hypotheses: the null hypothesis, which states that a simple model is sufficient 
to explain the data, against a complex hypothesis, which states that a higher-order model is required. 

To determine the best model for the Wage data, we will create 5 different polynomial models and run ANOVA. 
```{r poly_tests}
library(ISLR)
fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~poly(age,2), data=Wage)
fit.3 <- lm(wage~poly(age,3), data=Wage)
fit.4 <- lm(wage~poly(age,4), data=Wage)
fit.5 <- lm(wage~poly(age,5), data=Wage)

anova(fit.1, fit.2, fit.3, fit.4, fit.5)

```

Here, we see that the p-values for fits 2 and 3 are less than 0.05, which means the models are significant. However, the p-value for 
fit 4 is 0.051046. This means the fourth model is not significantly better than the third model. Therefore, we should use the third-order model. 

### Model Predictions
#### Polynomial Regression
Now that we have chosen model 3, let's use it to predict wage for some new list of ages. We'll first create a new set of ages to feed to the model. 
Then, we'll use the predict() function to calculate the predictions. 
```{r poly_predict}
agelims  = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
preds    = predict(fit.3, newdata=list(age=age.grid), se=TRUE)
```

Now, we want to determine the 95% confidence interval for our predictions, which corresponds to 2 standard deviations in each direction. 
```{r poly_conf}
se.bands = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
```

We can now plot the data, prediction, and confidence interval: 
```{r poly_plot}
par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim=agelims, cex =.5, col="darkgrey ")
title("Degree-3 Polynomial",outer=T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col=" blue", lty=3)
```

#### Polynomial Logistic Regression
The glm() function automatically returns predictions in logit form, so we need to transform them to the actual values. 
Other than that, the predictions and confidence intervals are computed and plotted in the same way. 

```{r poly_logistic_predict}
logist_preds = predict(logistic_fit,newdata=list(age=age.grid),se=T)
pfit = exp(logist_preds$fit)/(1+exp(logist_preds$fit))
se.bands.logit = cbind(logist_preds$fit +2* logist_preds$se.fit , logist_preds$fit -2*logist_preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))

plot(Wage$age ,I(Wage$wage >250),xlim=agelims ,type="n",ylim=c(0,.2))
points(jitter(Wage$age), I((Wage$wage >250)/5),cex=.5,pch ="|", col="darkgrey")
lines(age.grid ,pfit ,lwd=2, col ="blue")
matlines (age.grid ,se.bands ,lwd=1, col="blue",lty=3)
```

### Comprehensive Example:

#### Polynomial Regression
``` {r poly_full}
#pick best model 
fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~poly(age,2), data=Wage)
fit.3 <- lm(wage~poly(age,3), data=Wage)
fit.4 <- lm(wage~poly(age,4), data=Wage)
fit.5 <- lm(wage~poly(age,5), data=Wage)

anova(fit.1, fit.2, fit.3, fit.4, fit.5)

#fit.3 is the best, according to anova 
#make predictions on other data points
agelims  = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
preds    = predict(fit.3, newdata=list(age=age.grid), se=TRUE)

#compute confidence interval
se.bands = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)

#plot data, fit, 95% confidence interval
par(mfrow=c(1,1), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(Wage$age, Wage$wage, xlim=agelims, cex =.5, col="darkgrey ")
title("Degree-3 Polynomial",outer=T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col=" blue", lty=3)
````

#### Polynomial Logistic Regression
```{r poly_logistic_full}
#create the model 
logistic_fit <- glm(I(wage>250)~poly(age, 4), data=Wage, family=binomial)

#make predictions for new data 
logist_preds = predict(logistic_fit,newdata=list(age=age.grid),se=T)
pfit = exp(logist_preds$fit)/(1+exp(logist_preds$fit))
se.bands.logit = cbind(logist_preds$fit +2* logist_preds$se.fit , logist_preds$fit -2*logist_preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))

plot(Wage$age ,I(Wage$wage >250),xlim=agelims ,type="n",ylim=c(0,.2))
points(jitter(Wage$age), I((Wage$wage >250)/5),cex=.5,pch ="|", col="darkgrey")
lines(age.grid ,pfit ,lwd=2, col ="blue")
matlines (age.grid ,se.bands ,lwd=1, col="blue",lty=3)

```

--------------------------------------------------------------------------------

### Step Functions
Author: Alex Russett

#Summary and Description

In general **step functions** allow you to fit different areas of your data to different functions that
will allow them to fit the data better overall. This is stated in the book as avoiding imposing a 
**global structure** on data, and to instead break the data into **bins**, allowing us to fit a different constant to each bin.

To do this, we create cut points C(1), C(2)... ,C(k) in the range of X, then contruct K+! new variables. 

You can see example pictures on page 269 in chapter 7.2 of ISLR

### When do we use step functions? 

Step functions should ONLY be used when there are very natural and easy to identify break points 
in the X range of the data

Step Functions are commonly used among Neural Networks as well.


### Model Code and Inputs:
## Basics - Sample code not designed to be ran
```{r}
# Fitted Step function
lm(Y - cut(X,4))
# Fitted Logistic Regression
glm(Y - cut(X,4))
```
The above code chunk is choosing the data, and then the number of splits to employ! This is the entire applicaiton of the 
step function. 

Here it is in context!
### Full Comprehensive Example! 
## This example uses an LM function, but I will highlight when it fits a step function for analysis! 
```{r}
# Load library and attach data set
rm(list=ls())
library(ISLR)
attach(Wage)
```


```{r}
#using the lm() function, in order to predict
#wage using a fourth-degree polynomial in age: poly(age,4). The poly() command allows us to avoid having to write out a long formula with powers of age.
fit=lm(wage~poly(age ,4) ,data=Wage)
coef(summary(fit))
```

```{r}
#We can use poly() to obtain age, age^2, age^3, and age^4, if we prefer. We do this by adding the raw=TRUE argument to the poly() function
fit2 = lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
```
```{r}
# There are a few other ways to fit the model, showing the flexibility of the formula language in R:
fit2a = lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)

fit2b = lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b)
```
```{r}
#We now create a grid of values for age at which we want predictions, and then call the generic predict() function, specifying that we want standard errors as well
agelims =range(age)
age.grid=seq(from=agelims [1],to=agelims [2])
preds=predict (fit ,newdata =list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit +2* preds$se.fit,preds$fit -2* preds$se.fit)
```


```{r}
#Finally, we plot the data and add the fit from the degree-4 polynomial:
plot(age ,wage ,xlim=agelims ,cex =.5,col=" darkgrey ")
lines(age.grid ,preds$fit,lwd=2,col="blue")
matlines(age.grid ,se.bands ,lwd=1, col=" blue",lty=3)
```


```{r}
#To determine the simplest model which is sufficient to explain the relationship between wage and age we fit 5 models of increasing polynomial degrees:
fit.1 =lm(wage~age,data=Wage)
fit.2 =lm(wage~poly(age,2),data=Wage)
fit.3 =lm(wage~poly(age,3),data=Wage)
fit.4 =lm(wage~poly(age,4),data=Wage)
fit.5 =lm(wage~poly(age,5),data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```
We use the anova() function, which performs an analysis of variance (ANOVA, using an F-test) in order to test the null analysis of hypothesis that a model M1 is sufficient to explain the data against the variance alternative hypothesis that a more complex model M2 is required.

Result: Either a cubic or quadratic model (fit.3, fit.4) would be best for the data



```{r}
#Now we want to try to predict if an individual is making more than $250,000 per year:
fit = glm(I(wage>250)~poly(age,4), data=Wage, family=binomial)

#The expression wage > 250 evaluates to a logical variable of TRUE's and FALSE's, which are turned into 1's and 0's

preds = predict(fit,newdata=list(age=age.grid),se=T)


#The default prediction type for a glm() model is type="link", which is what we use here. This means we get predictions
#of the logit, so some transformations need to be done to obtain the correct confidence interval
pfit = exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit=cbind(preds$fit + 2*preds$se.fit, preds$fit - 2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))


```


```{r}
#To complete the right-hand side plot of figure 7.1:
plot(age,I(wage>250), xlim=agelims, type="n", ylim=c(0,.2))
points(jitter(age), I((wage>250)/5), cex=.5, pch="|", col="darkgrey")
lines(age.grid, pfit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)

```

### Step Function Implementation ###
 
```{r}
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
```



### Results and Final Thoughts

The function automatically chose breakpoints of 33.5, 49, and 64.5 years, but we could have chosen our own 
bins with the "breaks" option when splitting the data into bins. 

The cut() function returns an ordered categorical variable, which the lm() function then uses to create dummy variables
to regress upon.

Age less than 33.5 is left out, and the result of that intercept is the average salary for those under 33.5 years old. 



--------------------------------------------------------------------------------


## Regression Splines {.tabset}
Author: Andrew Tremblay & Thomas Trankle

### Summary/description:

**Regression Splines** are more flexible than polynomials and step functions. 

This is because the are actually an extension of the two. 

They involve dividing the range of *X* into *K* distinct regions. Within each region, 
a polynomial function is fit to the data. However, these polynomials are constrained so that 
they **join smoothly at the region boundaries, or knots**. Provided that the interval is
divided into enough regions, this can produce an extremely flexible fit.

The points where the coefficients change are called *knots*.

In general, if we place *K* different knots throughout the range of *X*, then we
will end up fitting *K + 1* different polynomials.
```{r, echo = F, cache=T}
knitr::include_graphics("./regKnots.png")
```

To make it a continuous function we add constraints to the model.
Each constraint that we impose on the piecewise polynomials effectively frees up one
degree of freedom.

Remember, there are degrees of freedom for the entire model(Total DF) and the 
specified degrees of freedom for the polynomial (df).
$$df = degree + knots$$
```{r, eval = F}
# Using the bs() function
bs(x , df = NULL, knots = NULL, degree = 3 #default
   intercept = False)
```

\centering
$$\textit{Total DF} = 1 + df = 1 + degree + knots$$

### When to employ this method/model:

Instead of fitting a high-degree polynomial over the entire range of *X*, *piecewise
polynomial regression* involves fitting separate low-degree polynomials over different 
regions of *X*.

We use this model when we need a high degree of flexibility.

Using more knots leads to a more flexible piecewise polynomial.




### Model Code and Inputs:

```{r}
library(ISLR)
library(splines)
library(boot)
```

To generate a regression spline, we use the bs() function from the *spline* package.
```{r, message=F}
# To ensure the data is inputted, we advise attaching the data
attach(Wage)

# generally, we need to either specify the df or number of knots
#bs(age, df = NULL, knots = NULL, degree = 3, intercept = FALSE,
#   Boundary.knots = range(x)) 

# The following code outlines fitting wage to age using a regression spline
# by defining the number of knots
regression.spline <- glm(wage ~ bs(age,
                                 knots=c(25,40,60))) #knot argument specifies 
                                                     #specific knot locations
                                                     # on axis

# If you don't know where to create the knots but know you want 3 knots in your
# model, then specify df and knot locations will be automatically generated.
regression.spline <- glm(wage~bs(age, df = 6)) #6 was used as default degree = 3 
                                               # df = degree + knots
                                               # 6= 3 + knots

#You cannot use knots to specify number of knots directly
# lm(Y ~ bs(X, knots=3) # error – will not give you 3 knots

# The Total DF for this model is 7
```

### Model Tests:

What is the best degree for our polynomial and how many knots should we use, or equivalently how many degrees of
freedom should our spline contain? One option is to try out different numbers of knots and see which produces the best looking curve. A somewhat
more objective approach is to use cross-validation.

There are two steps for this.

1.) Polynomial degree determination for our data using CV
```{r, message=F}
mse.error <- rep(0,12)
set.seed(5082)
for (i in 1:12){
  regSpline <- glm(wage ~ bs(age, degree = i), data = Wage)
  mse.error[i] <- cv.glm(Wage, regSpline, K=10)$delta[1]
}

plot(mse.error, xlab = 'Degrees of Freedom of Cubic Spline',ylab = 'MSE')
lines(mse.error, type = 'b',lty = 1)
stdev <- sd(mse.error[2:12])
abline(h= min(mse.error) + stdev, col = "red", lty = "dashed")
```

2.) Now we can use the best degree found above, determine the correct number of 
knots, if any.
```{r}
# METHOD 1 - look for the best model using CV whilst changing knots
error.vector <- rep(0,12) # create an error vector to store cv results

set.seed(5082)
for (i in 4:15 ){
  regression.spline <- glm(wage ~ bs(age, df = i, degree= 4))
  error.vector[i-3] <- cv.glm(Wage, regression.spline, K = 10)$delta[1] #The default is the average squared error function.(MSE)
}

# Plot Results 

plot(error.vector, xlab = 'Knots',ylab = '10-Fold CV Percentage Error')
lines(error.vector, type = 'b',lty = 1)
stdev <- sd(error.vector[1:12])
abline(h= min(error.vector) + stdev, col = "red", lty = "dashed")
```
So, for a degree-4 polynomial we will choose zero knots because this is the
most simple model whose CV error, for knots, lies 
within 1 SD error of the lowest cross validation error point.

### Model Improvements:

```{r}

```

### Comprehensive Example:

AUTO DATA
```{r, message=FALSE, warning=F}
library(ISLR)
attach(Auto)
error.vector.degree <- rep(0,10)
set.seed(111)
# First determine appropriate polynomial degree
for (i in 1:10 ){
  regression.spline <- glm(acceleration ~ bs(weight, degree = i), data = Auto)
  error.vector.degree[i] <- cv.glm(Auto, regression.spline, K=10)$delta[1] 
}
```

```{r}
plot(error.vector.degree, xlab = 'Degrees of Freedom',ylab = 'MSE')
lines(error.vector.degree, type = 'b',lty = 1)
stdev <- sd(error.vector.degree[1:9])
abline(h= min(error.vector.degree) + stdev, col = "red", lty = "dashed")

```

We examine the best degree for our model is 4.

```{r, message = F, warning = F}
error.vector.knots <- rep(0,10) # create an error vector to store cv results

set.seed(222)
for (i in 1:10 ){
  regression.spline <- glm(acceleration ~ bs(weight, df = i + 3,degree = 4))
  error.vector.knots[i] <- cv.glm(Auto, regression.spline, K = 10)$delta[1] 
}
```

```{r}
# Plot Results 

plot(error.vector.knots, xlab = 'Knots',ylab = '10-Fold CV Percentage Error')
lines(error.vector.knots, type = 'b',lty = 1)
stdev <- sd(error.vector.knots[1:10])
abline(h= min(error.vector.knots) + stdev, col = "red", lty = "dashed")
```

We get two Knots!!!!
Since the actual knots = Knots (x-axis) - 1

--------------------------------------------------------------------------------


## Smoothing Splines {.tabset}
Author: Emma Harrison

### Summary/description:
A smoothing spline is a natural cubic spline with a knot at every x. Can achieve perfect fit (RSS of 0) by going through every data point. 
Want a function that makes RSS small, but keeps the line smooth. Ensure that it’s smooth by: minimizing the tuning parameter. 

### When to employ this method/model:
One of the 4 non-linear model options. 
lots of flexibility
lots of degrees of freedom (one @ each knot = sample size n)

### Model Code and Inputs:

```{r,eval = F}
(9) Now let's fit smoothing splines with cross-validation
regression_fit=smooth.spline(x,y,df=12)   #use the first fit to set df
smooth_fit=smooth.spline(x,y, cv=TRUE)  #fits the training data with LOOCV
df= Degrees of freedom
cv= cross validation
(10) Display number of degrees of freedom in the cross-validated smoothing spline.
smooth_fit$df   #df produces splines at uniform knots

```

### Model Tests:

```{r,eval = F}
(11) Use the best model to predict y-values for the 50 new x's from step (7) above
ylims=range(y)
vector1<-seq(from=ylims[1],to=ylims[2], length.out = 50)
Visually inspect the plot of predicted values against X values
(12)	Plot the full sample's x's and y's.
xlims=range(x)
plot(x,y,xlim=xlims, ylim=ylims,cex=.5,col="darkgrey")
title("Smoothing Spline")

```

### Model Improvements:

```{r,eval = F}
N/A... would just use a different model
```

### Comprehensive Example:

```{r,eval = F}
> library(splines)
> fit=lm(wage∼bs(age ,knots=c(25,40,60) ),data=Wage)
> pred=predict (fit ,newdata =list(age=age.grid),se=T)
> plot(age ,wage ,col="gray")
> lines(age.grid ,pred$fit ,lwd=2)
> lines(age.grid ,pred$fit +2*pred$se ,lty="dashed ")
> lines(age.grid ,pred$fit -2*pred$se ,lty="dashed ")
> fit2=lm(wage∼ns(age ,df=4),data=Wage) 
> pred2=predict (fit2 ,newdata=list(age=age.grid),se=T)
> lines(age.grid , pred2$fit ,col="red",lwd=2)
> plot(age ,wage ,xlim=agelims ,cex =.5,col=" darkgrey ") 
> title("Smoothing Spline ") > fit=smooth .spline(age ,wage ,df=16) 
> fit2=smooth.spline (age ,wage ,cv=TRUE) 
> fit2$df
> lines(fit ,col="red",lwd =2)
> lines(fit2 ,col="blue",lwd=2) 
> legend ("topright ",legend=c("16 DF" ,"6.8 DF"), col=c("red","blue"),lty=1,lwd=2, cex =.8)

```
--------------------------------------------------------------------------------


## Local Regression {.tabset}
Author: Kayleigh Gillis

### Summary 
NOTE: Librarys ISLR, splines, game, and akima need to be installed. 

Local regression is an approach for fitting flexible non-linear functions. It involved computing the fit at a target point using a neighborhood of nearby training observations. To obtain the local regression fit at a new point, we need to fit a new weighted least squares regression model. It is someties called memory-based procedure because all of the training data is used each time a prediction is made.

### When to use it 

There are a few choices that are needed to be made. How to define the weighting function K, and whether linear, constant, or quadratic regression should be used. The most important choice is the span (s) which plays the role of a tuning parameter. The smaller the value, the more local the fit will be, the larger the span the more global the fit will be. Cross validation can be used to chose the span, or it can be specified directly. Local regression generalizes naturally when we want to fit models that have a pair of local vairables instead of one. Though too many can cause local regression to preform poorly. 

### Example 1:
```{r}
remove(list = ls())

library(ISLR)
attach(Wage)
library(splines)
# Use regression spline to fit wage to age
fit = lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)

# bs function generates entire matrix of basis functions for splines w #specified set of knots 
agelims = range(Wage$age)

# Grid of values for age that we want predictions
age.grid=seq(from=agelims [1],to=agelims [2])

# To preform local regression, use the loess() function 
plot(age, wage, xlim = agelims, cex = 0.5, col = "darkgrey")
title("Local Regression")
fit = loess(wage~age, span = 0.2, data = Wage)
fit2 = loess(wage~age, span = 0.5, data = Wage)
lines(age.grid ,predict (fit ,data.frame(age=age.grid)),
      col="red",lwd=2)
lines(age.grid ,predict (fit2 ,data.frame(age=age.grid)),
        col="blue",lwd=2)
legend ("topright ",legend=c("Span=0.2"," Span=0.5"),
          col=c("red","blue"),lty=1,lwd=2, cex =.8)


```

### Example 2
```{r}
# Local Regression as building blocks with GAM using lo()
library(gam)
gam.lo = gam(wage~s(year, df = 4) + lo(age, span = 0.7) + education, 
             data = Wage)
plot(gam.lo, se = TRUE, col = "green")
# We have used local regression on the age term with a span of 0.7
# The lo() function can create interactions before galling the gam() function 

gam.lo.i = gam(wage~lo(year, age, span = 0.5)+education, data = Wage)
# This fits a 2 term model. The first term is an interaction between year and age, by a local regression surface. 

## Plot the resulting 2 dim surface
library(akima)
plot(gam.lo.i)


```
--------------------------------------------------------------------------------

## GAM {.tabset}
Author: Akram Bijapuri
### Summary/description:

Generalized Additive Models allow us to fit a non-linear smoothing function (smoothing spline, natural splining, poly, etc.) to each of the variables(X) at one time.

GAMS maintain additivity which means you can add different functions to each variable while adding to examine each effect on an X variable when you make a function change or add another variable while holding all of the other variables constant. This allows for easy comparison through multiple models.

In basic terms: you can apply smoothing to one variable ("s(Blah, df=4)"), then add polynomial function to another Xvariable ("poly(Blah2,3)"), then add local regression to another xVariable ("lo(Blah3,df)") etc. You can keep doing this, or you can start anova testing to see which one has the best significance (meaning you should use that). This allows you to find best model.

You can use combine qualitative and quantitative variables(X) with GAM. In the below examples, I am using age(quantitative) and education(qualitative) to predict wage.

You can also observe qualitative and quantitative responses(Y) with GAM. For example, you can identify use it to identify predicted Wage as a quantitative value, or as a classifier "is wage greater than $250 at with these known variable(s). Prediction = Yes/No".

### When to employ this method/model:

GAM allows a lot of flexibility in working with multiple non-linear terms. We can cycle X variables through to find best combination.

Since it is additive, we can examine how the effect on each transformation of X. We can compare multiple models with ANOVA tests to identify best model with the best combination of transformations.

Negative: It can only be additive. In analyzing data with multiple models, it is difficult to measure all the interactions and so some may be missed. Determining degrees of freedom in every variable in data with large metadata is difficult.


### Model Code and Inputs:
```{r setupWork}
library(ISLR)
library(splines)
library(gam)
Wage = Wage
##I am predicting Wage with:
### smoothing spline of age with 6 dF (s() is the smoothing spline function)
### Smoothing spline of year with 6 dF
### Education is factor variable but we want to add it, no need to smooth. Step Function is applied.
levels(Wage$education)

gam1=gam(wage~s(age,df=6)+s(year,df=6)+education,data=Wage)

## Remember, GAM is taking only these 3 variables into account because we added those.

par(mfrow=c(1,3))
plot(gam1,se=T)

#Salaries increase with age then decrease
#there seemed to be an general increase for year collected, except around 2007 (recession)
#Advanced Degrees are significant in increasing wage

```
### Model Test:
```{r test1, tidy=TRUE}

#Let's observe qualitative output here  (Will do quantitative in comprehensive example)
#By using I() we are converting the response to be above $250 Yes/No (1/0)
logitgam1=gam(I(wage>250)~s(age,df=4)+s(year,df=4)+education,data=Wage, family=binomial)
par(mfrow=c(1,3))
plot(logitgam1,se=T)

#THe above is very similar, except the predicted variable is the logit. Basically if it over 0, it does meet above $250k. Also note, No one with less than a HS Grad education makes over $250k in our data, so none is predicted. 

#Note:Plot for Year variable we can see that the error bands are quiet wide and broad.This might be an indication that our Non linear function fitted for ‘Year’ variable is not significant.

#Let's test to see if changing year to be linear matters:
logitgam2<-gam(I(wage >250) ~ s(age ,df=4)+ year + education , data =Wage, family = binomial)
par(mfrow=c(1,3))
plot(logitgam2,se=T)
#see in the 2nd plot the year is a straight line now. That's because we models added a linear regression model this time. Let's find out if it's better that way.
```

### Model Improvements:
```{r AnovaModel, tidy=TRUE}

anova(logitgam1,logitgam2)

#With the p value of .8242, there is no significant improvement in choosing the smoothing application on the year, instead of just the linear function is fine. it is also simpler to choose the one without the year and get good results. 

```
### Comprehensive Example:

In the below example you can see the comparison in 3 different GAM models. We are reviewing whether we should include "Year", and if we should, should we use a linear function or a smoothing function on it.



```{r ComprehensiveTest, tidy=TRUE}
gam.m1=gam(wage~s(age,5) +education ,data=Wage)
gam.m2=gam(wage~s(age,5)+year+education ,data=Wage)
gam.m3=gam(wage~s(age ,5)+s(year,4)+education ,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")

predWage=unname(predict(gam.m2,newdata=Wage))
head(Wage$wage);head(predWage)

#There is a significance in choosing the 2nd model as shown in the ftest

#Since Model2 has the best f-test compared to model1, we can use this model as our best model. This model is doing a better job predicting the variance than model1 and having the year is significant. 
#However, model3 shows hat is not significant in comparison to model1 and we shouldn't include 

#We can then use this prediction model to predict y values for wage. Note we modeled train data to test data #here, but only using 3 of the predictor variables.

```

--------------------------------------------------------------------------------


## Tree Classification and Regression {.tabset}
Author: Ian Lawson 

### Summary/description:

In the simplest terms, trees classification and regression are just methods of decision trees to classify data. Tree classification splits data and the nodes (categories after the splits) categorize data as one category or another. Tree regression does essentially the same thing.  While it's technically regression, each node has a specific value that the model categorizes it as. Splits are made by trying to determine what kind of split will make the most pure node. For example, if you were trying to categorize loan applicants as likely to default on their loan or not, and you realized most people with a credit score below 650 defaulted on their loans, you would make a split at a credit score of 650. The purity of the two nodes you create depend on how mixed the split is. If when you split in the example above and 50 people with scores below 650 default on their loan while only 2 people did not, it would be a fairly pure node. On the other hand, if 26 defaulted and 26 did not, it would be very impure and not a good split. 

There are two ways to determine impurity: find the Gini Impurity, or find the Entropy. Dr. Chung primarily talked about Gini, but a quick note on Entropy: values for Entropy are between 0 and 1 with 0 being a pure node and 1 having the most possible impurity. For Gini, the range is 0 to 0.5 with 0 being pure and 0.5 being the most impure. To calculate the Gini impurity use one minus the sum of the probability squared of each potential classification. To go with our earlier example, the Gini impurity would be 1 - (sum of probability of defaulting)^2 - (sum of probability of not defaulting)^2 Lastly, when making cuts, you look at how much information is gained from the cut. This is the difference in Gini impurity between the parent and child cut. The more information gain, the better the cut. 


### When to employ this method/model:

Generally, Tree Regression and Classification are not the best. Later sections that use bagging and random forest can be decent, but tree methods are very prone to high error rates and overfitting. The value comes from the simplicity of the model. They are very useful when creating a model that's basic for people to understand. Generally, try to avoid using these. 

### Model Code and Inputs for both Regression and Classification:

```{r,eval = F}
#tree models use the tree library
library(tree)
#models follow the general ~ formula
model<-tree(y_variable~x_variables,data=your_data)
#to view the summary
summary(model)
#to view a more comprehensive summary
model
#to view a plot of the model
plot(model)
text(model, pretty=0)
```

### Model Tests, Classification:

```{r,eval = F}
#predict using test data
prediction<-predict(model,test_data,type = 'class')
#make a confusion matrix
table(prediction, test_data$y_values)
#calculate test error using the confusion matrix
test_error<-number_of_values_correctly_predicted/number_of_all_values
```

### Model Tests, Regression:

```{r,eval = F}
#predict using test data (note there is no type='class')
prediction<-predict(model,test_data)
#find MSE
MSE<-mean((prediction - test_data$y_values)^2)
#find RMSE
RMSE<-sqrt(mean((prediction - test_data$y_values)^2))
```

### Model Improvements, Classification:

```{r,eval = F}
#use cross validation 
cvModel<-cv.tree(model, FUN = prune.misclass)
#plot the model and choose the fewest number of nodes within on standard deviation of the node with the lowest error
plot(cvModel$size,cvModel$dev, type = 'b')
#remake the model with the best number of nodes
pruneModel<-prune.misclass(model,best=best_number_of_nodes)

```

### Model Improvements, Regression:

```{r,eval = F}
#use cross validation 
cvModel<-cv.tree(model)
#plot the model and choose the fewest number of nodes within on standard deviation of the node with the lowest error
plot(cvModel$size,cvModel$dev, type = 'b')
#remake the model with the best number of nodes
pruneModel<-prune.misclass(model,best=best_number_of_nodes)

```

### Comprehensive Example, Regression:

Fit the Tree:

We fit a regression tree to the Boston Housing Data, which is available through the MASS package in R studio and include 14 variables and 506 observations.
First, we split data half and half into training and test sets , then fit the regression tree model on the training data only.

```{r}
library(tree)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv~., Boston, subset = train)
summary(tree.boston)
```

Plot Tree: 

Notice that although the tree grown to full depth has 7 leaves, only four of the variables (rm, lstat, crim and age) have been used to construct this tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. Now, we plot the tree.

```{r}
plot(tree.boston)
text(tree.boston,pretty=0)
```

The rm variable measures the average number of rooms per dwelling, and the lstat variable measures the percentage of individuals with lower socioeconomic status. The tree shows that higher values of rm correspond to higher house values and lower values of rm correspond to cheap houses.

Preform Cross-Validation: 

Now we use 10-fold cross validation by using cv.tree() function in order to determine the optimal level of tree complexity. This will help us decide whether pruning the tree will improve performance.

```{r}
cv.boston=cv.tree(tree.boston)
cv.boston
```

Plot the Cross-Validation: 

```{r}
plot(cv.boston$size,cv.boston$dev,type='b')
```

Through the plot, we can find that the most complex tree is selected by cross-validation. However if we wanted to prune the tree, we would use the prune.tree() function as the following.

Make model by using Best Pruned Tree: 

```{r}
prune.boston=prune.tree(tree.boston,best=5)
summary(prune.boston)
plot(prune.boston)
text(prune.boston,pretty=0)
```

Find the Test Error Rate: 

We use the unpruned tree to make predictions on the test set to keep the cross-validation results.
```{r}
yhat = predict(tree.boston,newdata=Boston[-train,])
boston.test = Boston[-train,"medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
sqrt(mean((yhat-boston.test)^2))
```
We got the result that the test set MSE for the regression tree is 35.29, and the square root is around 5.940 which means that this model gives predictions that are within around $5,940 of the true median home value.

### Comprehensive Example, Classification:

Load the Data:

The OJ data contains 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. There are 17 characteristics of the customer and product recorded in the data. The data set is already cleaned as shown in the 'str' function, so no data cleaning is required. 

```{r}
library(ISLR)
data(OJ)
str(OJ)
```

Split the Data: 

Instead of using the book's method, the data is split using the caret package. The training data contains 80% of the data while the remaining 20% is saved for testing. 

```{r}
library(caret)
set.seed(420)
d_divide<-createDataPartition(OJ$Purchase,p=.8,list = FALSE)
train<-OJ[d_divide,]
test<-OJ[-d_divide,]
```


Fit the Model:

To fit the tree classification model, use the tree library. As seen in the model summary, there are 10 terminal nodes and a training error rate of 15.87%.


```{r}
library(tree)
model<-tree(Purchase~.,data=train)
summary(model)
```

Display Detailed Results:

Printing the model shows the break down of each node in the tree. In order it displays the split criterion, the number of observations in that branch, the deviance, the overall prediction for the branch, and the fraction of observations that take on the prediction. It also denotes if a node is a terminal node or not. 

```{r}
model
```

Plot the Tree:

Plotting the tree shows a visualization of the different branches in the tree. It does not go as in-depth as printing the model, but it does show the split criteria. 

```{r}
plot(model)
text(model,pretty = 0)
```

Use the Test Data and Create a Confusion Matrix:

Using the test data to make a prediction yields a test error rate of 16.9%. This is slightly higher than the training error rate, which is generally expected. There were 36 misclassifications and 177 valid classifications.  

```{r}
prediction<-predict(model,test,type = 'class')
table(prediction, test$Purchase)
cat("Test Error Rate:",toString((26+10)/length(test$Purchase)))
```

Use Cross Validation to Find Optimal Tree Size:

To cross validate the tree model, the cv.tree function is used. Setting FUN to prune.misclass ensures that the training error rate is used as the criteria for model improvement. Displaying the results of the model shows the deviation at each number of terminal nodes. 

```{r}
cvModel<-cv.tree(model, FUN = prune.misclass)
cvModel
```

Plot the CV Results:

Plotting the cross validation results shows that using 4 terminal nodes yields the smallest deviation/error rate for the model. This differs from our original model that used 10 terminal nodes. Not only is the error rate lower, less nodes and branches means there is a lower chance of overfitting. 

```{r}
plot(cvModel$size,cvModel$dev, type = 'b')
```

Make a Model Using the Best Pruned Tree:

Using the number of terminal nodes found above, a pruned model is created using the prune.misclass function. The plot shows the branches for a 4 terminal node model. 

```{r}
pruneModel<-prune.misclass(model,best=4)
plot(pruneModel)
text(pruneModel,pretty=0)
```

Find the New Test Error Rate:

Using the pruned model yields different results from the original model. The training error rate was slightly higher at 18.09% as opposed to the original 15.87%. The test error rate fell by 2.82% from the original test error rate of 16.9% to the pruned test error rate of 14.08%. 

```{r}
summary(pruneModel)
prunePred<-predict(pruneModel,test,type = 'class')
table(prunePred, test$Purchase)
cat("Test Error Rate:",toString((13+17)/length(test$Purchase)))
```
--------------------------------------------------------------------------------


## Bagging and Random Forest {.tabset}
Author: Itty Bitty Witty

### Summary/description:
Bagging, random forests and boosting use trees as building blocks to construct more powerful prediction models.

Bagging, also known as bootstrap aggregation, is a general purpose procedure for reducing the variance of a statistical learning model, usually decision trees. Some decision trees, notably in Section 8.1 of the book, suffer from high variance. Bagging helps reduce that variance. 

In bagging, we generate B different bootstrapped training data sets and train our method on the bth training set to get $\hat{f}^{*b} (x)$, and average the predictions. The full formula looks like this:

$\hat{f}_{bag} (x)$=$\frac{1}{B}$ $\sum_{b=1}^{B} \hat{f}^{*b} (x)$

Bagging can be used for regression models, but it is most useful with decision trees.

Something to note - the number of trees $B$ is not a critical parameter with bagging; using a large $B$ will not lead to overfitting.

### When to employ this method/model:

** For quantitative variables ** : Do above

** For qualitative variables ** : Simplest approach is to record the class predicted by each of the B trees, and take a majority vote: the most common occurring class among B predictions.

To setup our bagging code, we need some initial regression tree code.

### Model Code and Inputs:
```{r}
library(MASS)
library(randomForest)
library(tree)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset = train)
summary(tree.boston)

# Regression Tree
tree(formula = medv~., data = Boston, subset = train)
plot(tree.boston)
text(tree.boston, pretty = 0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type = 'b')

# prune tree
prune.boston=prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)

# use unpruned tree to make predictions on the test set
yhat=predict(tree.boston, newdata = Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)

# Need randomForest package to setup a random forest 

library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data = Boston, subset = train, mtry=13, importance=TRUE)

# The argument "mtry=13" lets us know that all 13 predictors should be considered for each split within the tree

bag.boston

# The percentage of variance explained is 85.17%. 
# B = 500. If argument isn't specified, ntrees will default to 500.
# The number of variables tried at each split: 13
## If mtry is not specified, it will default to p/3 for mtry for regression trees and sqrt(6) for classification
```

### Model Tests:

```{r}
yhat.bag=predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

### Model Improvements:

A model can be improved by changing the mtry argument and how many variables you want build a random forest on or by altering the amount of trees the forest is made of using the ntrees argument.

```{r}
set.seed(1)
rf.boston=randomForest(medv~., data = Boston, subset = train, mtry=6, importance=TRUE, ntrees = 25)
```

The dataset that isn't used to train the model is then your "Out-Of-Bag" Sample, or your test dataset. Estimate the accuracy of the random forest using the Out-of-Bag sample.

```{r}
yhat.rf=predict(rf.boston, newdata = Boston[-train,]) ## newdata is your Out-of-Bag data
mean((yhat.rf-boston.test)^2)
```

With the 'importance = TRUE' argument, you can then see which features are most, or least, important to the random forest. Call this using varImpPlot()

```{r}
varImpPlot(rf.boston)
```


### Comprehensive Example:

In this exercise, we'll use boosting to predict **Salary** in the Hitters data set.

First thing we want to do is load the dataset and remove any observations where salary is missing.


```{r}
library(MASS)
library(tree)
hitters <- ISLR::Hitters
hitters<-hitters[-which(is.na(hitters$Salary)), ]
hitters$Salary<-log(hitters$Salary)
```
 

We'll then create a training set consisting of the first 200 hitters, and a test set consisting of the remaining observations.


```{r}
train<-1:200
hitters.train <- hitters[train,]
hitters.test <- hitters[-train,]
```

Then we use randomForest including all 19 variables. It will default to 500 trees.

```{r}
library(randomForest)
set.seed(1)
bag.hitters= randomForest(Salary~.,data=hitters.train,
                            mtry=19,importance =TRUE) #use all 19 variables for bagging
yhat.bag <- predict(bag.hitters , newdata=hitters.test)
mean((yhat.bag - hitters.test$Salary)^2)
```

Call the varImpPlot to see which variables are most important to this random forest

```{r}
varImpPlot(bag.hitters)
```
